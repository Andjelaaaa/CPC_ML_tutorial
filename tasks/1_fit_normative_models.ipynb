{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "name": "1_fit_normative_models.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b64f505-ad16-437a-94de-2646f35ae55f"
      },
      "source": [
        "## Estimating lifespan normative models\n",
        "\n",
        "This notebook provides a complete walkthrough for an analysis of normative modelling in a large sample as described in the accompanying paper. Note that this script is provided principally for completeness (e.g. to assist in fitting normative models to new datasets). All pre-estimated normative models are already provided.\n",
        "\n",
        "First, if necessary, we install PCNtoolkit (note: this tutorial requires at least version 0.20)"
      ],
      "id": "4b64f505-ad16-437a-94de-2646f35ae55f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "84ec2ca6-c0a2-4abf-8f05-29edc9e0fa24",
        "outputId": "fb1d0bef-61f3-4f49-e333-366f0c19eb60"
      },
      "source": [
        "# Make sure to click the restart runtime button at the \n",
        "# bottom of this code blocks' output (after you run the cell)\n",
        "!pip install pcntoolkit==0.20"
      ],
      "id": "84ec2ca6-c0a2-4abf-8f05-29edc9e0fa24",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pcntoolkit==0.20 in /usr/local/lib/python3.7/dist-packages (0.20)\n",
            "Requirement already satisfied: sphinx-tabs in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (3.2.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (0.0)\n",
            "Requirement already satisfied: bspline in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (0.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (1.15.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (1.19.5)\n",
            "Requirement already satisfied: nibabel>=2.5.1 in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (3.0.2)\n",
            "Collecting argparse\n",
            "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: arviz==0.11.0 in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (0.11.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (3.2.2)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (1.9.0+cu102)\n",
            "Requirement already satisfied: theano==1.0.5 in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (1.0.5)\n",
            "Requirement already satisfied: pymc3<=3.9.3,>=3.8 in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (3.9.3)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (1.1.5)\n",
            "Requirement already satisfied: setuptools>=38.4 in /usr/local/lib/python3.7/dist-packages (from arviz==0.11.0->pcntoolkit==0.20) (57.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from arviz==0.11.0->pcntoolkit==0.20) (21.0)\n",
            "Requirement already satisfied: typing-extensions<4,>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from arviz==0.11.0->pcntoolkit==0.20) (3.7.4.3)\n",
            "Requirement already satisfied: netcdf4 in /usr/local/lib/python3.7/dist-packages (from arviz==0.11.0->pcntoolkit==0.20) (1.5.7)\n",
            "Requirement already satisfied: xarray>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from arviz==0.11.0->pcntoolkit==0.20) (0.18.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pcntoolkit==0.20) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pcntoolkit==0.20) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pcntoolkit==0.20) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pcntoolkit==0.20) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.3->pcntoolkit==0.20) (2018.9)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from pymc3<=3.9.3,>=3.8->pcntoolkit==0.20) (0.5.1)\n",
            "Requirement already satisfied: h5py>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from pymc3<=3.9.3,>=3.8->pcntoolkit==0.20) (3.1.0)\n",
            "Requirement already satisfied: fastprogress>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pymc3<=3.9.3,>=3.8->pcntoolkit==0.20) (1.0.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.7.0->pymc3<=3.9.3,>=3.8->pcntoolkit==0.20) (1.5.2)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.7/dist-packages (from netcdf4->arviz==0.11.0->pcntoolkit==0.20) (1.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->pcntoolkit==0.20) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->pcntoolkit==0.20) (1.0.1)\n",
            "Requirement already satisfied: sphinx<5,>=2 in /usr/local/lib/python3.7/dist-packages (from sphinx-tabs->pcntoolkit==0.20) (4.2.0)\n",
            "Requirement already satisfied: docutils~=0.16.0 in /usr/local/lib/python3.7/dist-packages (from sphinx-tabs->pcntoolkit==0.20) (0.16)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from sphinx-tabs->pcntoolkit==0.20) (2.6.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (2.0.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (1.0.2)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (1.1.5)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (2.23.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (1.2.0)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (1.0.3)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (2.11.3)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (1.0.2)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (2.9.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (2.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (3.0.4)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "909c3b45-ad46-4e6d-8732-dc5ac68488c6"
      },
      "source": [
        "Then we import the required libraries"
      ],
      "id": "909c3b45-ad46-4e6d-8732-dc5ac68488c6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGQhP2LbElmI",
        "outputId": "cf0a227e-2524-419d-d703-3ed7e354cd4a"
      },
      "source": [
        "! git clone https://github.com/saigerutherford/CPC_ML_tutorial.git"
      ],
      "id": "DGQhP2LbElmI",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CPC_ML_tutorial'...\n",
            "remote: Enumerating objects: 341, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/59)\u001b[K\rremote: Counting objects:   3% (2/59)\u001b[K\rremote: Counting objects:   5% (3/59)\u001b[K\rremote: Counting objects:   6% (4/59)\u001b[K\rremote: Counting objects:   8% (5/59)\u001b[K\rremote: Counting objects:  10% (6/59)\u001b[K\rremote: Counting objects:  11% (7/59)\u001b[K\rremote: Counting objects:  13% (8/59)\u001b[K\rremote: Counting objects:  15% (9/59)\u001b[K\rremote: Counting objects:  16% (10/59)\u001b[K\rremote: Counting objects:  18% (11/59)\u001b[K\rremote: Counting objects:  20% (12/59)\u001b[K\rremote: Counting objects:  22% (13/59)\u001b[K\rremote: Counting objects:  23% (14/59)\u001b[K\rremote: Counting objects:  25% (15/59)\u001b[K\rremote: Counting objects:  27% (16/59)\u001b[K\rremote: Counting objects:  28% (17/59)\u001b[K\rremote: Counting objects:  30% (18/59)\u001b[K\rremote: Counting objects:  32% (19/59)\u001b[K\rremote: Counting objects:  33% (20/59)\u001b[K\rremote: Counting objects:  35% (21/59)\u001b[K\rremote: Counting objects:  37% (22/59)\u001b[K\rremote: Counting objects:  38% (23/59)\u001b[K\rremote: Counting objects:  40% (24/59)\u001b[K\rremote: Counting objects:  42% (25/59)\u001b[K\rremote: Counting objects:  44% (26/59)\u001b[K\rremote: Counting objects:  45% (27/59)\u001b[K\rremote: Counting objects:  47% (28/59)\u001b[K\rremote: Counting objects:  49% (29/59)\u001b[K\rremote: Counting objects:  50% (30/59)\u001b[K\rremote: Counting objects:  52% (31/59)\u001b[K\rremote: Counting objects:  54% (32/59)\u001b[K\rremote: Counting objects:  55% (33/59)\u001b[K\rremote: Counting objects:  57% (34/59)\u001b[K\rremote: Counting objects:  59% (35/59)\u001b[K\rremote: Counting objects:  61% (36/59)\u001b[K\rremote: Counting objects:  62% (37/59)\u001b[K\rremote: Counting objects:  64% (38/59)\u001b[K\rremote: Counting objects:  66% (39/59)\u001b[K\rremote: Counting objects:  67% (40/59)\u001b[K\rremote: Counting objects:  69% (41/59)\u001b[K\rremote: Counting objects:  71% (42/59)\u001b[K\rremote: Counting objects:  72% (43/59)\u001b[K\rremote: Counting objects:  74% (44/59)\u001b[K\rremote: Counting objects:  76% (45/59)\u001b[K\rremote: Counting objects:  77% (46/59)\u001b[K\rremote: Counting objects:  79% (47/59)\u001b[K\rremote: Counting objects:  81% (48/59)\u001b[K\rremote: Counting objects:  83% (49/59)\u001b[K\rremote: Counting objects:  84% (50/59)\u001b[K\rremote: Counting objects:  86% (51/59)\u001b[K\rremote: Counting objects:  88% (52/59)\u001b[K\rremote: Counting objects:  89% (53/59)\u001b[K\rremote: Counting objects:  91% (54/59)\u001b[K\rremote: Counting objects:  93% (55/59)\u001b[K\rremote: Counting objects:  94% (56/59)\u001b[K\rremote: Counting objects:  96% (57/59)\u001b[K\rremote: Counting objects:  98% (58/59)\u001b[K\rremote: Counting objects: 100% (59/59)\u001b[K\rremote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 341 (delta 17), reused 44 (delta 8), pack-reused 282\u001b[K\n",
            "Receiving objects: 100% (341/341), 7.94 MiB | 28.22 MiB/s, done.\n",
            "Resolving deltas: 100% (140/140), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d451c106-08e2-4f5b-baf9-da240768e68b"
      },
      "source": [
        "# we need to be in the CPC_ML_tutorial folder when we import the libraries in the code block below,\n",
        "# because there is a function called nm_utils that is in this folder that we need to import\n",
        "import os\n",
        "os.chdir('/content/CPC_ML_tutorial/')"
      ],
      "id": "d451c106-08e2-4f5b-baf9-da240768e68b",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83c494d3-6ebd-4cde-aff0-8fc9344374dd"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from pcntoolkit.normative import estimate, predict, evaluate\n",
        "from pcntoolkit.util.utils import compute_MSLL, create_design_matrix\n",
        "from nm_utils import calibration_descriptives, remove_bad_subjects, load_2d"
      ],
      "id": "83c494d3-6ebd-4cde-aff0-8fc9344374dd",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9822cc19-48e9-428b-8c5e-e059fd2d23f7"
      },
      "source": [
        "Now, we configure the locations in which the data are stored. You will need to configure this for your specific installation\n",
        "\n",
        "**Notes:** \n",
        "- The data are assumed to be in CSV format and will be loaded as pandas dataframes\n",
        "- Generally the raw data will be in a different location to the analysis\n",
        "- The data can have arbitrary columns but some are required by the script, i.e. 'age', 'sex' and 'site', plus the phenotypes you wish to estimate (see below)"
      ],
      "id": "9822cc19-48e9-428b-8c5e-e059fd2d23f7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7da01c88-7033-498b-a811-79ad58e8c17a"
      },
      "source": [
        "# where the raw data are stored\n",
        "data_dir = '/content/CPC_ML_tutorial/data/'\n",
        "\n",
        "# where the analysis takes place\n",
        "root_dir = '/content/CPC_ML_tutorial/'\n",
        "out_dir = os.path.join(root_dir,'models','test')\n",
        "\n",
        "# create the output directory if it does not already exist\n",
        "os.makedirs(out_dir, exist_ok=True)"
      ],
      "id": "7da01c88-7033-498b-a811-79ad58e8c17a",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01141f19-a960-4823-baad-8604975304c3"
      },
      "source": [
        "Now we load the data. \n",
        "\n",
        "We will load one pandas dataframe for the training set and one dataframe for the test set. We also configrure a list of site ids."
      ],
      "id": "01141f19-a960-4823-baad-8604975304c3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "850fee6b-421f-41d9-8fd6-7e1dafbf0e9f"
      },
      "source": [
        "df_tr = pd.read_csv(os.path.join(data_dir,'train_data.csv'), index_col=0) \n",
        "df_te = pd.read_csv(os.path.join(data_dir,'test_data.csv'), index_col=0)\n",
        "\n",
        "# extract a list of unique site ids from the training set\n",
        "site_ids =  sorted(set(df_tr['site'].to_list()))"
      ],
      "id": "850fee6b-421f-41d9-8fd6-7e1dafbf0e9f",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29f9593a-d3c9-4d08-a877-8794203c0001"
      },
      "source": [
        "### Configure which models to fit\n",
        "\n",
        "Next, we load the image derived phenotypes (IDPs) which we will process in this analysis. This is effectively just a list of columns in your dataframe. Here we estimate normative models for the left hemisphere, right hemisphere and cortical structures."
      ],
      "id": "29f9593a-d3c9-4d08-a877-8794203c0001"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7438ef7e-9340-4f13-8d57-816918923705"
      },
      "source": [
        "# load the idps to process\n",
        "with open(os.path.join(data_dir,'phenotypes_lh.txt')) as f:\n",
        "    idp_ids_lh = f.read().splitlines()\n",
        "with open(os.path.join(data_dir,'phenotypes_rh.txt')) as f:\n",
        "    idp_ids_rh = f.read().splitlines()\n",
        "with open(os.path.join(data_dir,'phenotypes_sc.txt')) as f:\n",
        "    idp_ids_sc = f.read().splitlines()\n",
        "\n",
        "# we choose here to process all idps\n",
        "#idp_ids = idp_ids_lh + idp_ids_rh + idp_ids_sc\n",
        "\n",
        "# we could also just specify a list of IDPs\n",
        "idp_ids = ['lh_MeanThickness_thickness', 'rh_MeanThickness_thickness']"
      ],
      "id": "7438ef7e-9340-4f13-8d57-816918923705",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d791db6-8fe5-450c-88eb-84a390b8753a"
      },
      "source": [
        "### Configure model parameters\n",
        "\n",
        "Now, we configure some parameters for the regression model we use to fit the normative model. Here we will use a 'warped' Bayesian linear regression model. To model non-Gaussianity, we select a sin arcsinh warp and to model non-linearity, we stick with the default value for the basis expansion (a cubic b-spline basis set with 5 knot points). Since we are sticking with the default value, we do not need to specify any parameters for this, but we do need to specify the limits. We choose to pad the input by a few years either side of the input range. We will also set a couple of options that control the estimation of the model\n",
        "\n",
        "For further details about the likelihood warping approach, see [Fraza et al 2021](https://www.biorxiv.org/content/10.1101/2021.04.05.438429v1)."
      ],
      "id": "5d791db6-8fe5-450c-88eb-84a390b8753a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IYl-eg2xGWE",
        "outputId": "d0dccc82-4d52-4141-8ec7-7e9d58d455e8"
      },
      "source": [
        "df_tr['age'].describe()"
      ],
      "id": "0IYl-eg2xGWE",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1163.000000\n",
              "mean       40.388727\n",
              "std        17.235724\n",
              "min        18.000000\n",
              "25%        28.000000\n",
              "50%        32.643395\n",
              "75%        53.000000\n",
              "max        87.000000\n",
              "Name: age, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e44e257c-676e-49d8-89ec-657e506c3b74"
      },
      "source": [
        "# which data columns do we wish to use as covariates? \n",
        "cols_cov = ['age','sex']\n",
        "\n",
        "# which warping function to use? We can set this to None in order to fit a vanilla Gaussian noise model\n",
        "warp =  'WarpSinArcsinh'\n",
        "\n",
        "# limits for cubic B-spline basis \n",
        "# check the min & max ages of the dataframes, add 5 to the max \n",
        "# and subtract 5 from the min and adjust these variables accordingly\n",
        "xmin = 13 \n",
        "xmax = 93\n",
        "\n",
        "# Do we want to force the model to be refit every time? \n",
        "# When training normative model from scratch (not re-using a pre-trained model), \n",
        "# this variable should be = True\n",
        "force_refit = True \n",
        "\n",
        "# Absolute Z treshold above which a sample is considered to be an outlier (without fitting any model)\n",
        "outlier_thresh = 7"
      ],
      "id": "e44e257c-676e-49d8-89ec-657e506c3b74",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "896842d7-8913-4137-9d86-4757c42bcf1b"
      },
      "source": [
        "### Fit the models\n",
        "\n",
        "Now we fit the models. This involves looping over the IDPs we have selected. We will use a module from PCNtoolkit to set up the design matrices, containing the covariates, fixed effects for site and nonlinear basis expansion. "
      ],
      "id": "896842d7-8913-4137-9d86-4757c42bcf1b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4e9b50c-574b-4e2c-a511-cc444db4393e",
        "outputId": "73b828b6-2371-4015-f351-7d663d1c65d7"
      },
      "source": [
        "for idp_num, idp in enumerate(idp_ids): \n",
        "    print('Running IDP', idp_num, idp, ':')\n",
        "   \n",
        "    # set output dir \n",
        "    idp_dir = os.path.join(out_dir, idp)\n",
        "    os.makedirs(os.path.join(idp_dir), exist_ok=True)\n",
        "    os.chdir(idp_dir)\n",
        "    \n",
        "    # extract the response variables for training and test set\n",
        "    y_tr = df_tr[idp].to_numpy() \n",
        "    y_te = df_te[idp].to_numpy()\n",
        "    \n",
        "    # remove gross outliers and implausible values\n",
        "    yz_tr = (y_tr - np.mean(y_tr)) / np.std(y_tr)\n",
        "    yz_te = (y_te - np.mean(y_te)) / np.std(y_te)\n",
        "    nz_tr = np.bitwise_and(np.abs(yz_tr) < outlier_thresh, y_tr > 0)\n",
        "    nz_te = np.bitwise_and(np.abs(yz_te) < outlier_thresh, y_te > 0)\n",
        "    y_tr = y_tr[nz_tr]\n",
        "    y_te = y_te[nz_te]\n",
        "    \n",
        "    # write out the response variables for training and test\n",
        "    resp_file_tr = os.path.join(idp_dir, 'resp_tr.txt')\n",
        "    resp_file_te = os.path.join(idp_dir, 'resp_te.txt') \n",
        "    np.savetxt(resp_file_tr, y_tr)\n",
        "    np.savetxt(resp_file_te, y_te)\n",
        "        \n",
        "    # configure the design matrix\n",
        "    X_tr = create_design_matrix(df_tr[cols_cov].loc[nz_tr], \n",
        "                                site_ids = df_tr['site'].loc[nz_tr],\n",
        "                                basis = 'bspline', \n",
        "                                xmin = xmin, \n",
        "                                xmax = xmax)\n",
        "    X_te = create_design_matrix(df_te[cols_cov].loc[nz_te], \n",
        "                                site_ids = df_te['site'].loc[nz_te], \n",
        "                                all_sites=site_ids,\n",
        "                                basis = 'bspline', \n",
        "                                xmin = xmin, \n",
        "                                xmax = xmax)\n",
        "\n",
        "    # configure and save the covariates\n",
        "    cov_file_tr = os.path.join(idp_dir, 'cov_bspline_tr.txt')\n",
        "    cov_file_te = os.path.join(idp_dir, 'cov_bspline_te.txt')\n",
        "    np.savetxt(cov_file_tr, X_tr)\n",
        "    np.savetxt(cov_file_te, X_te)\n",
        "\n",
        "    if not force_refit and os.path.exists(os.path.join(idp_dir, 'Models', 'NM_0_0_estimate.pkl')):\n",
        "        print('Making predictions using a pre-existing model...')\n",
        "        suffix = 'predict'\n",
        "        \n",
        "        # Make prdictsion with test data\n",
        "        predict(cov_file_te, \n",
        "                alg='blr', \n",
        "                respfile=resp_file_te, \n",
        "                model_path=os.path.join(idp_dir,'Models'),\n",
        "                outputsuffix=suffix)\n",
        "    else:\n",
        "        print('Estimating the normative model...')\n",
        "        estimate(cov_file_tr, resp_file_tr, testresp=resp_file_te, \n",
        "                 testcov=cov_file_te, alg='blr', optimizer = 'l-bfgs-b', \n",
        "                 savemodel=True, warp=warp, warp_reparam=True)\n",
        "        suffix = 'estimate'"
      ],
      "id": "a4e9b50c-574b-4e2c-a511-cc444db4393e",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running IDP 0 lh_MeanThickness_thickness :\n",
            "Estimating the normative model...\n",
            "Processing data in /content/CPC_ML_tutorial/models/test/lh_MeanThickness_thickness/resp_tr.txt\n",
            "Estimating model  1 of 1\n",
            "configuring BLR ( order 1 )\n",
            "Using default hyperparameters\n",
            "Saving model meta-data...\n",
            "Evaluating the model ...\n",
            "Writing outputs ...\n",
            "Running IDP 1 rh_MeanThickness_thickness :\n",
            "Estimating the normative model...\n",
            "Processing data in /content/CPC_ML_tutorial/models/test/rh_MeanThickness_thickness/resp_tr.txt\n",
            "Estimating model  1 of 1\n",
            "configuring BLR ( order 1 )\n",
            "Using default hyperparameters\n",
            "Saving model meta-data...\n",
            "Evaluating the model ...\n",
            "Writing outputs ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "925f77cf-c873-4047-91ac-50b9571704fd"
      },
      "source": [
        "### Compute error metrics\n",
        "\n",
        "In this section we compute the following error metrics for all IDPs (all evaluated on the test set):\n",
        "\n",
        "- Negative log likelihood (NLL)\n",
        "- Explained variance (EV)\n",
        "- Mean standardized log loss (MSLL)\n",
        "- Bayesian information Criteria (BIC)\n",
        "- Skew and Kurtosis of the Z-distribution"
      ],
      "id": "925f77cf-c873-4047-91ac-50b9571704fd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "2e9d7500-4f46-4ee1-9756-81758ae5b1d1",
        "outputId": "b02c5b72-3d74-4c92-be2c-e37e6eed0ff6"
      },
      "source": [
        "# initialise dataframe we will use to store quantitative metrics \n",
        "blr_metrics = pd.DataFrame(columns = ['eid', 'NLL', 'EV', 'MSLL', 'BIC','Skew','Kurtosis'])\n",
        "\n",
        "for idp_num, idp in enumerate(idp_ids): \n",
        "    idp_dir = os.path.join(out_dir, idp)\n",
        "    \n",
        "    # load the predictions and true data. We use a custom function that ensures 2d arrays\n",
        "    # equivalent to: y = np.loadtxt(filename); y = y[:, np.newaxis]\n",
        "    yhat_te = load_2d(os.path.join(idp_dir, 'yhat_' + suffix + '.txt'))\n",
        "    s2_te = load_2d(os.path.join(idp_dir, 'ys2_' + suffix + '.txt'))\n",
        "    y_te = load_2d(os.path.join(idp_dir, 'resp_te.txt'))\n",
        "    \n",
        "    with open(os.path.join(idp_dir,'Models', 'NM_0_0_estimate.pkl'), 'rb') as handle:\n",
        "        nm = pickle.load(handle) \n",
        "    \n",
        "    # compute error metrics\n",
        "    if warp is None:\n",
        "        metrics = evaluate(y_te, yhat_te)  \n",
        "        \n",
        "        # compute MSLL manually as a sanity check\n",
        "        y_tr_mean = np.array( [[np.mean(y_tr)]] )\n",
        "        y_tr_var = np.array( [[np.var(y_tr)]] )\n",
        "        MSLL = compute_MSLL(y_te, yhat_te, s2_te, y_tr_mean, y_tr_var)         \n",
        "    else:\n",
        "        warp_param = nm.blr.hyp[1:nm.blr.warp.get_n_params()+1] \n",
        "        W = nm.blr.warp\n",
        "        \n",
        "        # warp predictions\n",
        "        med_te = W.warp_predictions(np.squeeze(yhat_te), np.squeeze(s2_te), warp_param)[0]\n",
        "        med_te = med_te[:, np.newaxis]\n",
        "       \n",
        "        # evaluation metrics\n",
        "        metrics = evaluate(y_te, med_te)\n",
        "        \n",
        "        # compute MSLL manually\n",
        "        y_te_w = W.f(y_te, warp_param)\n",
        "        y_tr_w = W.f(y_tr, warp_param)\n",
        "        y_tr_mean = np.array( [[np.mean(y_tr_w)]] )\n",
        "        y_tr_var = np.array( [[np.var(y_tr_w)]] )\n",
        "        MSLL = compute_MSLL(y_te_w, yhat_te, s2_te, y_tr_mean, y_tr_var)     \n",
        "    \n",
        "    Z = np.loadtxt(os.path.join(idp_dir, 'Z_' + suffix + '.txt'))\n",
        "    [skew, sdskew, kurtosis, sdkurtosis, semean, sesd] = calibration_descriptives(Z)\n",
        "    \n",
        "    BIC = len(nm.blr.hyp) * np.log(y_tr.shape[0]) + 2 * nm.neg_log_lik\n",
        "    \n",
        "    blr_metrics.loc[len(blr_metrics)] = [idp, nm.neg_log_lik, metrics['EXPV'][0], \n",
        "                                         MSLL[0], BIC, skew, kurtosis]\n",
        "    \n",
        "display(blr_metrics)\n",
        "\n",
        "blr_metrics.to_csv(os.path.join(out_dir,'blr_metrics.csv'))"
      ],
      "id": "2e9d7500-4f46-4ee1-9756-81758ae5b1d1",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eid</th>\n",
              "      <th>NLL</th>\n",
              "      <th>EV</th>\n",
              "      <th>MSLL</th>\n",
              "      <th>BIC</th>\n",
              "      <th>Skew</th>\n",
              "      <th>Kurtosis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>lh_MeanThickness_thickness</td>\n",
              "      <td>-930.749803</td>\n",
              "      <td>0.300769</td>\n",
              "      <td>-0.151972</td>\n",
              "      <td>-1833.268015</td>\n",
              "      <td>0.181322</td>\n",
              "      <td>0.831460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>rh_MeanThickness_thickness</td>\n",
              "      <td>-908.602580</td>\n",
              "      <td>0.407773</td>\n",
              "      <td>-0.202958</td>\n",
              "      <td>-1788.973569</td>\n",
              "      <td>0.233710</td>\n",
              "      <td>0.596452</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          eid         NLL  ...      Skew  Kurtosis\n",
              "0  lh_MeanThickness_thickness -930.749803  ...  0.181322  0.831460\n",
              "1  rh_MeanThickness_thickness -908.602580  ...  0.233710  0.596452\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCpzbIwGxVWj"
      },
      "source": [
        "blr_metrics['EV'].describe()"
      ],
      "id": "NCpzbIwGxVWj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et7L-t9RJl75"
      },
      "source": [
        "blr_metrics['MSLL'].describe()"
      ],
      "id": "Et7L-t9RJl75",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s25LW4QuJqfW"
      },
      "source": [
        "blr_metrics['EV'].hist()"
      ],
      "id": "s25LW4QuJqfW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBhUMsojJu5J"
      },
      "source": [
        "blr_metrics['MSLL'].hist()"
      ],
      "id": "mBhUMsojJu5J",
      "execution_count": null,
      "outputs": []
    }
  ]
}