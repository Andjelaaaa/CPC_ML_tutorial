{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "apply_normative_models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d8fb4c8-4360-4fdc-b0a2-e1c2e22bd8f9"
      },
      "source": [
        "## Using lifespan models to make predictions on new data\n",
        "\n",
        "This notebook shows how to apply the coefficients from pre-estimated normative models to new data. This can be done in two different ways: (i) using a new set of data derived from the same sites used to estimate the model and (ii) on a completely different set of sites. In the latter case, we also need to estimate the site effect, which requires some calibration/adaptation data. As an illustrative example, we use a dataset derived from the [1000 functional connectomes project](https://www.nitrc.org/forum/forum.php?thread_id=2907&forum_id=1383) and adapt the learned model to make predictions on these data. \n",
        "\n",
        "First, if necessary, we install PCNtoolkit (note: this tutorial requires at least version 0.20)"
      ],
      "id": "2d8fb4c8-4360-4fdc-b0a2-e1c2e22bd8f9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8d05182a-5346-49d2-bfbf-fd3769ecc061",
        "outputId": "22c20334-2291-4553-8e95-9477882ce5c5"
      },
      "source": [
        "!pip install pcntoolkit==0.20"
      ],
      "id": "8d05182a-5346-49d2-bfbf-fd3769ecc061",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pcntoolkit==0.20 in /usr/local/lib/python3.7/dist-packages (0.20)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (1.9.0+cu102)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (0.0)\n",
            "Collecting argparse\n",
            "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: nibabel>=2.5.1 in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (3.0.2)\n",
            "Requirement already satisfied: sphinx-tabs in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (3.2.0)\n",
            "Requirement already satisfied: theano==1.0.5 in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (1.0.5)\n",
            "Requirement already satisfied: arviz==0.11.0 in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (0.11.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (1.1.5)\n",
            "Requirement already satisfied: pymc3<=3.9.3,>=3.8 in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (3.9.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (3.2.2)\n",
            "Requirement already satisfied: bspline in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (0.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pcntoolkit==0.20) (1.15.0)\n",
            "Requirement already satisfied: xarray>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from arviz==0.11.0->pcntoolkit==0.20) (0.18.2)\n",
            "Requirement already satisfied: setuptools>=38.4 in /usr/local/lib/python3.7/dist-packages (from arviz==0.11.0->pcntoolkit==0.20) (57.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from arviz==0.11.0->pcntoolkit==0.20) (21.0)\n",
            "Requirement already satisfied: typing-extensions<4,>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from arviz==0.11.0->pcntoolkit==0.20) (3.7.4.3)\n",
            "Requirement already satisfied: netcdf4 in /usr/local/lib/python3.7/dist-packages (from arviz==0.11.0->pcntoolkit==0.20) (1.5.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pcntoolkit==0.20) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pcntoolkit==0.20) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pcntoolkit==0.20) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pcntoolkit==0.20) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.3->pcntoolkit==0.20) (2018.9)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from pymc3<=3.9.3,>=3.8->pcntoolkit==0.20) (0.5.1)\n",
            "Requirement already satisfied: fastprogress>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from pymc3<=3.9.3,>=3.8->pcntoolkit==0.20) (1.0.0)\n",
            "Requirement already satisfied: h5py>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from pymc3<=3.9.3,>=3.8->pcntoolkit==0.20) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.7.0->pymc3<=3.9.3,>=3.8->pcntoolkit==0.20) (1.5.2)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.7/dist-packages (from netcdf4->arviz==0.11.0->pcntoolkit==0.20) (1.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->pcntoolkit==0.20) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->pcntoolkit==0.20) (1.0.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from sphinx-tabs->pcntoolkit==0.20) (2.6.1)\n",
            "Requirement already satisfied: docutils~=0.16.0 in /usr/local/lib/python3.7/dist-packages (from sphinx-tabs->pcntoolkit==0.20) (0.16)\n",
            "Requirement already satisfied: sphinx<5,>=2 in /usr/local/lib/python3.7/dist-packages (from sphinx-tabs->pcntoolkit==0.20) (4.2.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (1.0.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (1.2.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (1.0.3)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (2.11.3)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (2.1.0)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (2.23.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (2.9.1)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (1.1.5)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (2.0.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (0.7.12)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (2.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->sphinx<5,>=2->sphinx-tabs->pcntoolkit==0.20) (3.0.4)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6utoL2n53IBr"
      },
      "source": [
        "! rm -rf braincharts"
      ],
      "id": "6utoL2n53IBr",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V6JFzpdJ43R",
        "outputId": "385682e4-f053-4cc6-d6b4-7e018eede435"
      },
      "source": [
        "! git clone https://github.com/predictive-clinical-neuroscience/braincharts.git"
      ],
      "id": "5V6JFzpdJ43R",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'braincharts'...\n",
            "remote: Enumerating objects: 232, done.\u001b[K\n",
            "remote: Counting objects: 100% (232/232), done.\u001b[K\n",
            "remote: Compressing objects: 100% (173/173), done.\u001b[K\n",
            "remote: Total 232 (delta 106), reused 169 (delta 55), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (232/232), 11.33 MiB | 30.54 MiB/s, done.\n",
            "Resolving deltas: 100% (106/106), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5ZET1btKF6J"
      },
      "source": [
        "# we need to be in the scripts folder when we import the libraries in the code block below,\n",
        "# because there is a function called nm_utils that is in the scripts folder that we need to import\n",
        "import os\n",
        "os.chdir('/content/braincharts/scripts/')"
      ],
      "id": "_5ZET1btKF6J",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2227bc7-e798-470a-99bc-33561ce4511b"
      },
      "source": [
        "Now we import the required libraries"
      ],
      "id": "b2227bc7-e798-470a-99bc-33561ce4511b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff661cf2-7d80-46bb-bcfb-1650a93eed3d"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from pcntoolkit.normative import estimate, predict, evaluate\n",
        "from pcntoolkit.util.utils import compute_MSLL, create_design_matrix\n",
        "from nm_utils import remove_bad_subjects, load_2d"
      ],
      "id": "ff661cf2-7d80-46bb-bcfb-1650a93eed3d",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFxsGN-KgfE0"
      },
      "source": [
        "We need to unzip the models. "
      ],
      "id": "TFxsGN-KgfE0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OvpUTaIgekS"
      },
      "source": [
        "os.chdir('/content/braincharts/models/')"
      ],
      "id": "0OvpUTaIgekS",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBP9CEVcgsjT",
        "outputId": "da6f94f5-fff3-4ebb-aee1-45ddd0af0210"
      },
      "source": [
        "ls"
      ],
      "id": "WBP9CEVcgsjT",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lifespan_12K_57sites_mqc2_train.zip  lifespan_29K_82sites_train.zip\n",
            "lifespan_12K_59sites_mqc_train.zip   lifespan_57K_82sites.zip\n",
            "lifespan_23K_57sites_mqc2.zip        README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is47bTl_guD4"
      },
      "source": [
        "# we will use the biggest sample as our training set (approx. N=57000 subjects from 82 sites)\n",
        "! unzip lifespan_57K_82sites.zip"
      ],
      "id": "is47bTl_guD4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "802b1da6-04cc-4310-af81-f50d38c3e653"
      },
      "source": [
        "Next, we configure some basic variables, like where we want the analysis to be done and which model we want to use.\n",
        "\n",
        "**Note:** We maintain a list of site ids for each dataset, which describe the site names in the training and test data (`site_ids_tr` and `site_ids_te`), plus also the adaptation data . The training site ids are provided as a text file in the distribution and the test ids are extracted automatically from the pandas dataframe (see below). If you use additional data from the sites (e.g. later waves from ABCD), it may be necessary to adjust the site names to match the names in the training set. See the accompanying [paper](https://www.biorxiv.org/content/10.1101/2021.08.08.455487v2) for more details."
      ],
      "id": "802b1da6-04cc-4310-af81-f50d38c3e653"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26b35c64-41fd-4ecd-bf6e-3e7b34a67279"
      },
      "source": [
        "# which model do we wish to use?\n",
        "model_name = 'lifespan_57K_82sites'\n",
        "site_names = 'site_ids_82sites.txt'\n",
        "\n",
        "# where the analysis takes place\n",
        "root_dir = '/content/braincharts'\n",
        "out_dir = os.path.join(root_dir, 'models', model_name)\n",
        "\n",
        "# load a set of site ids from this model. This must match the training data\n",
        "with open(os.path.join(root_dir,'docs', site_names)) as f:\n",
        "    site_ids_tr = f.read().splitlines()"
      ],
      "id": "26b35c64-41fd-4ecd-bf6e-3e7b34a67279",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dbaebd7-4f86-47d8-82a5-1776eb96690f"
      },
      "source": [
        "### Download test dataset\n",
        "\n",
        "As mentioned above, to demonstrate this tool we will use a test dataset derived from the FCON 1000 dataset. We provide a prepackaged training/test split of these data in the required format (also after removing sites with only a few data points), [here](https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo/tree/main/data). you can get these data by running the following commmands:"
      ],
      "id": "8dbaebd7-4f86-47d8-82a5-1776eb96690f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60f72165-9b2f-4248-ba72-1a1f9683d280",
        "outputId": "7f665ae9-4bac-4b95-e733-d063624d24ea"
      },
      "source": [
        "os.chdir(root_dir)\n",
        "!wget -nc https://raw.githubusercontent.com/saigerutherford/CPC_ML_tutorial/master/data/fcon1000_tr.csv\n",
        "!wget -nc https://raw.githubusercontent.com/saigerutherford/CPC_ML_tutorial/master/data/fcon1000_te.csv"
      ],
      "id": "60f72165-9b2f-4248-ba72-1a1f9683d280",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-17 13:06:07--  https://raw.githubusercontent.com/saigerutherford/CPC_ML_tutorial/master/data/fcon1000_tr.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 786940 (768K) [text/plain]\n",
            "Saving to: ‘fcon1000_tr.csv’\n",
            "\n",
            "fcon1000_tr.csv     100%[===================>] 768.50K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-09-17 13:06:07 (16.5 MB/s) - ‘fcon1000_tr.csv’ saved [786940/786940]\n",
            "\n",
            "--2021-09-17 13:06:07--  https://raw.githubusercontent.com/saigerutherford/CPC_ML_tutorial/master/data/fcon1000_te.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 917069 (896K) [text/plain]\n",
            "Saving to: ‘fcon1000_te.csv’\n",
            "\n",
            "fcon1000_te.csv     100%[===================>] 895.58K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-09-17 13:06:08 (18.6 MB/s) - ‘fcon1000_te.csv’ saved [917069/917069]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aab54a5-2579-48d8-a81b-bbd34cea1213"
      },
      "source": [
        "### Load test data\n",
        "\n",
        "Now we load the test data and remove some subjects that may have poor scan quality. This asssesment is based on the Freesurfer Euler characteristic as described in the papers below. \n",
        "\n",
        "**Note:** For the purposes of this tutorial, we make predictions for all sites in the FCON 1000 dataset, but two of them were also included in the training data (named 'Baltimore' and 'NewYork_a'). In this case, this will only slightly bias the accuracy, but in order to replicate the results in the paper, it would be necessary to additionally remove these sites from the test dataframe.\n",
        "\n",
        "**References**\n",
        "- [Kia et al 2021](https://www.biorxiv.org/content/10.1101/2021.05.28.446120v1.abstract)\n",
        "- [Rosen et al 2018](https://www.sciencedirect.com/science/article/abs/pii/S1053811917310832?via%3Dihub)"
      ],
      "id": "3aab54a5-2579-48d8-a81b-bbd34cea1213"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "262d429a-160b-4ba3-9ba4-9acc195bc644",
        "outputId": "e38c0a03-3f44-463b-e385-ec01eafb660a"
      },
      "source": [
        "test_data = os.path.join(root_dir, 'fcon1000_te.csv')\n",
        "\n",
        "df_te = pd.read_csv(test_data, index_col=0)\n",
        "\n",
        "# remove some bad subjects\n",
        "df_te, bad_sub = remove_bad_subjects(df_te, df_te)\n",
        "\n",
        "# extract a list of unique site ids from the test set\n",
        "site_ids_te =  sorted(set(df_te['site'].to_list()))"
      ],
      "id": "262d429a-160b-4ba3-9ba4-9acc195bc644",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18 subjects are removed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c636509a-8b12-43f1-811c-08cb22640be2"
      },
      "source": [
        "### Load adaptation data\n",
        "\n",
        "If the data you wish to make predictions for is not derived from the same scanning sites as those in the trainig set, it is necessary to learn the site effect so that we can account for it in the predictions. In order to do this in an unbiased way, we use a separate dataset, which we refer to as 'adaptation' data. This must contain data for all the same sites as in the test dataset and we assume these are coded in the same way, based on a the 'sitenum' column in the dataframe. "
      ],
      "id": "c636509a-8b12-43f1-811c-08cb22640be2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53551023-aff6-4934-ad2d-d77bc63c562d",
        "outputId": "b59cc4e3-3646-47b7-eff8-0abb60dce75e"
      },
      "source": [
        "adaptation_data = os.path.join(root_dir, 'fcon1000_tr.csv')\n",
        "\n",
        "df_ad = pd.read_csv(adaptation_data, index_col=0)\n",
        "\n",
        "# remove some bad subjects\n",
        "df_ad, bad_sub = remove_bad_subjects(df_ad, df_ad)\n",
        "\n",
        "# extract a list of unique site ids from the test set\n",
        "site_ids_ad =  sorted(set(df_ad['site'].to_list()))\n",
        "\n",
        "if not all(elem in site_ids_ad for elem in site_ids_te):\n",
        "    print('Warning: some of the testing sites are not in the adaptation data')"
      ],
      "id": "53551023-aff6-4934-ad2d-d77bc63c562d",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 subjects are removed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f73e30e-c693-44b8-98c6-52b71b577ea8"
      },
      "source": [
        "### Configure which models to fit\n",
        "\n",
        "Now, we configure which imaging derived phenotypes (IDPs) we would like to process. This is just a list of column names in the dataframe we have loaded above. \n",
        "\n",
        "We could load the whole set i.e., all phenotypes for which we have models for (188 brain regions)."
      ],
      "id": "4f73e30e-c693-44b8-98c6-52b71b577ea8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b48e104c-cbac-4ae2-8377-cd3ff80162fd"
      },
      "source": [
        "# load the list of idps for left and right hemispheres, plus subcortical regions\n",
        "with open(os.path.join(root_dir,'docs','phenotypes_lh.txt')) as f:\n",
        "    idp_ids_lh = f.read().splitlines()\n",
        "with open(os.path.join(root_dir,'docs','phenotypes_rh.txt')) as f:\n",
        "    idp_ids_rh = f.read().splitlines()\n",
        "with open(os.path.join(root_dir,'docs','phenotypes_sc.txt')) as f:\n",
        "    idp_ids_sc = f.read().splitlines()\n",
        "\n",
        "# we choose here to process all idps\n",
        "idp_ids = idp_ids_lh + idp_ids_rh + idp_ids_sc"
      ],
      "id": "b48e104c-cbac-4ae2-8377-cd3ff80162fd",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "280731ad-47d8-43e2-8cb5-4eccfd9f3f81"
      },
      "source": [
        "... or alternatively, we could just specify a list of the brain regions we are interested in. "
      ],
      "id": "280731ad-47d8-43e2-8cb5-4eccfd9f3f81"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b74d75f-77a5-474a-9c9b-29aab1ce53a2"
      },
      "source": [
        "idp_ids = [ 'Left-Thalamus-Proper', 'Left-Lateral-Ventricle', 'rh_MeanThickness_thickness']"
      ],
      "id": "8b74d75f-77a5-474a-9c9b-29aab1ce53a2",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56ee1f7f-8684-4f1c-b142-a68176407029"
      },
      "source": [
        "### Configure covariates \n",
        "\n",
        "Now, we configure some parameters to fit the model. First, we choose which columns of the pandas dataframe contain the covariates (age and sex). The site parameters are configured automatically later on by the `configure_design_matrix()` function, when we loop through the IDPs in the list\n",
        "\n",
        "The supplied coefficients are derived from a 'warped' Bayesian linear regression model, which uses a nonlinear warping function to model non-Gaussianity (`sinarcsinh`) plus a non-linear basis expansion (a cubic b-spline basis set with 5 knot points, which is the default value in the PCNtoolkit package). Since we are sticking with the default value, we do not need to specify any parameters for this, but we do need to specify the limits. We choose to pad the input by a few years either side of the input range. We will also set a couple of options that control the estimation of the model\n",
        "\n",
        "For further details about the likelihood warping approach, see the accompanying paper and [Fraza et al 2021](https://www.biorxiv.org/content/10.1101/2021.04.05.438429v1)."
      ],
      "id": "56ee1f7f-8684-4f1c-b142-a68176407029"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62312b8e-4972-4238-abf9-87d9bb33cc10"
      },
      "source": [
        "# which data columns do we wish to use as covariates? \n",
        "cols_cov = ['age','sex']\n",
        "\n",
        "# limits for cubic B-spline basis \n",
        "xmin = -5 \n",
        "xmax = 110\n",
        "\n",
        "# Absolute Z treshold above which a sample is considered to be an outlier (without fitting any model)\n",
        "outlier_thresh = 7"
      ],
      "id": "62312b8e-4972-4238-abf9-87d9bb33cc10",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42bc1072-e9ed-4f2a-9fdd-cbd626a61542"
      },
      "source": [
        "### Make predictions\n",
        "\n",
        "This will make predictions for each IDP separately. This is done by extracting a column from the dataframe (i.e. specifying the IDP as the response variable) and saving it as a numpy array. Then, we configure the covariates, which is a numpy data array having the number of rows equal to the number of datapoints in the test set. The columns are specified as follows: \n",
        "\n",
        "- A global intercept (column of ones)\n",
        "- The covariate columns (here age and sex, coded as 0=female/1=male)\n",
        "- Dummy coded columns for the sites in the training set (one column per site)\n",
        "- Columns for the basis expansion (seven columns for the default parameterisation)\n",
        "\n",
        "Once these are saved as numpy arrays in ascii format (as here) or (alternatively) in pickle format, these are passed as inputs to the `predict()` method in the PCNtoolkit normative modelling framework. These are written in the same format to the location specified by `idp_dir`. At the end of this step, we have a set of predictions and Z-statistics for the test dataset that we can take forward to further analysis.\n",
        "\n",
        "Note that when we need to make predictions on new data, the procedure is more involved, since we need to prepare, process and store covariates, response variables and site ids for the adaptation data. "
      ],
      "id": "42bc1072-e9ed-4f2a-9fdd-cbd626a61542"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07b7471b-c334-464f-8273-b409b7acaac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5345b37-8335-47c6-c962-47d17a41c384"
      },
      "source": [
        "for idp_num, idp in enumerate(idp_ids): \n",
        "    print('Running IDP', idp_num, idp, ':')\n",
        "    idp_dir = os.path.join(out_dir, idp)\n",
        "    os.chdir(idp_dir)\n",
        "    \n",
        "    # extract and save the response variables for the test set\n",
        "    y_te = df_te[idp].to_numpy()\n",
        "    \n",
        "    # save the variables\n",
        "    resp_file_te = os.path.join(idp_dir, 'resp_te.txt') \n",
        "    np.savetxt(resp_file_te, y_te)\n",
        "        \n",
        "    # configure and save the design matrix\n",
        "    cov_file_te = os.path.join(idp_dir, 'cov_bspline_te.txt')\n",
        "    X_te = create_design_matrix(df_te[cols_cov], \n",
        "                                site_ids = df_te['site'],\n",
        "                                all_sites = site_ids_tr,\n",
        "                                basis = 'bspline', \n",
        "                                xmin = xmin, \n",
        "                                xmax = xmax)\n",
        "    np.savetxt(cov_file_te, X_te)\n",
        "    \n",
        "    # check whether all sites in the test set are represented in the training set\n",
        "    if all(elem in site_ids_tr for elem in site_ids_te):\n",
        "        print('All sites are present in the training data')\n",
        "        \n",
        "        # just make predictions\n",
        "        yhat_te, s2_te, Z = predict(cov_file_te, \n",
        "                                    alg='blr', \n",
        "                                    respfile=resp_file_te, \n",
        "                                    model_path=os.path.join(idp_dir,'Models'))\n",
        "    else:\n",
        "        print('Some sites missing from the training data. Adapting model')\n",
        "        \n",
        "        # save the covariates for the adaptation data\n",
        "        X_ad = create_design_matrix(df_ad[cols_cov], \n",
        "                                    site_ids = df_ad['site'],\n",
        "                                    all_sites = site_ids_tr,\n",
        "                                    basis = 'bspline', \n",
        "                                    xmin = xmin, \n",
        "                                    xmax = xmax)\n",
        "        cov_file_ad = os.path.join(idp_dir, 'cov_bspline_ad.txt')          \n",
        "        np.savetxt(cov_file_ad, X_ad)\n",
        "        \n",
        "        # save the responses for the adaptation data\n",
        "        resp_file_ad = os.path.join(idp_dir, 'resp_ad.txt') \n",
        "        y_ad = df_ad[idp].to_numpy()\n",
        "        np.savetxt(resp_file_ad, y_ad)\n",
        "       \n",
        "        # save the site ids for the adaptation data\n",
        "        sitenum_file_ad = os.path.join(idp_dir, 'sitenum_ad.txt') \n",
        "        site_num_ad = df_ad['sitenum'].to_numpy(dtype=int)\n",
        "        np.savetxt(sitenum_file_ad, site_num_ad)\n",
        "        \n",
        "        # save the site ids for the test data \n",
        "        sitenum_file_te = os.path.join(idp_dir, 'sitenum_te.txt')\n",
        "        site_num_te = df_te['sitenum'].to_numpy(dtype=int)\n",
        "        np.savetxt(sitenum_file_te, site_num_te)\n",
        "         \n",
        "        yhat_te, s2_te, Z = predict(cov_file_te, \n",
        "                                    alg = 'blr', \n",
        "                                    respfile = resp_file_te, \n",
        "                                    model_path = os.path.join(idp_dir,'Models'),\n",
        "                                    adaptrespfile = resp_file_ad,\n",
        "                                    adaptcovfile = cov_file_ad,\n",
        "                                    adaptvargroupfile = sitenum_file_ad,\n",
        "                                    testvargroupfile = sitenum_file_te)"
      ],
      "id": "07b7471b-c334-464f-8273-b409b7acaac2",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running IDP 0 lh_G&S_frontomargin_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 1 lh_G&S_occipital_inf_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 2 lh_G&S_paracentral_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 3 lh_G&S_subcentral_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 4 lh_G&S_transv_frontopol_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 5 lh_G&S_cingul-Ant_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 6 lh_G&S_cingul-Mid-Ant_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 7 lh_G&S_cingul-Mid-Post_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 8 lh_G_cingul-Post-dorsal_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 9 lh_G_cingul-Post-ventral_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 10 lh_G_cuneus_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 11 lh_G_front_inf-Opercular_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 12 lh_G_front_inf-Orbital_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 13 lh_G_front_inf-Triangul_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 14 lh_G_front_middle_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 15 lh_G_front_sup_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 16 lh_G_Ins_lg&S_cent_ins_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 17 lh_G_insular_short_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 18 lh_G_occipital_middle_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 19 lh_G_occipital_sup_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 20 lh_G_oc-temp_lat-fusifor_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 21 lh_G_oc-temp_med-Lingual_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 22 lh_G_oc-temp_med-Parahip_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 23 lh_G_orbital_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 24 lh_G_pariet_inf-Angular_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 25 lh_G_pariet_inf-Supramar_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 26 lh_G_parietal_sup_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 27 lh_G_postcentral_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 28 lh_G_precentral_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 29 lh_G_precuneus_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 30 lh_G_rectus_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 31 lh_G_subcallosal_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 32 lh_G_temp_sup-G_T_transv_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 33 lh_G_temp_sup-Lateral_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 34 lh_G_temp_sup-Plan_polar_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 35 lh_G_temp_sup-Plan_tempo_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 36 lh_G_temporal_inf_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 37 lh_G_temporal_middle_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 38 lh_Lat_Fis-ant-Horizont_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 39 lh_Lat_Fis-ant-Vertical_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 40 lh_Lat_Fis-post_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 41 lh_Pole_occipital_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 42 lh_Pole_temporal_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 43 lh_S_calcarine_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 44 lh_S_central_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 45 lh_S_cingul-Marginalis_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 46 lh_S_circular_insula_ant_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 47 lh_S_circular_insula_inf_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 48 lh_S_circular_insula_sup_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 49 lh_S_collat_transv_ant_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 50 lh_S_collat_transv_post_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 51 lh_S_front_inf_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 52 lh_S_front_middle_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 53 lh_S_front_sup_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 54 lh_S_interm_prim-Jensen_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 55 lh_S_intrapariet&P_trans_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 56 lh_S_oc_middle&Lunatus_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 57 lh_S_oc_sup&transversal_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 58 lh_S_occipital_ant_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 59 lh_S_oc-temp_lat_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 60 lh_S_oc-temp_med&Lingual_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 61 lh_S_orbital_lateral_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 62 lh_S_orbital_med-olfact_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 63 lh_S_orbital-H_Shaped_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 64 lh_S_parieto_occipital_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 65 lh_S_pericallosal_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 66 lh_S_postcentral_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 67 lh_S_precentral-inf-part_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 68 lh_S_precentral-sup-part_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 69 lh_S_suborbital_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 70 lh_S_subparietal_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 71 lh_S_temporal_inf_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 72 lh_S_temporal_sup_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 73 lh_S_temporal_transverse_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 74 lh_MeanThickness_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 75 rh_G&S_frontomargin_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 76 rh_G&S_occipital_inf_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 77 rh_G&S_paracentral_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 78 rh_G&S_subcentral_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 79 rh_G&S_transv_frontopol_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 80 rh_G&S_cingul-Ant_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 81 rh_G&S_cingul-Mid-Ant_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 82 rh_G&S_cingul-Mid-Post_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 83 rh_G_cingul-Post-dorsal_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 84 rh_G_cingul-Post-ventral_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 85 rh_G_cuneus_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 86 rh_G_front_inf-Opercular_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 87 rh_G_front_inf-Orbital_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 88 rh_G_front_inf-Triangul_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 89 rh_G_front_middle_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 90 rh_G_front_sup_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 91 rh_G_Ins_lg&S_cent_ins_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 92 rh_G_insular_short_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 93 rh_G_occipital_middle_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 94 rh_G_occipital_sup_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 95 rh_G_oc-temp_lat-fusifor_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 96 rh_G_oc-temp_med-Lingual_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 97 rh_G_oc-temp_med-Parahip_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 98 rh_G_orbital_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 99 rh_G_pariet_inf-Angular_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 100 rh_G_pariet_inf-Supramar_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 101 rh_G_parietal_sup_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 102 rh_G_postcentral_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 103 rh_G_precentral_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 104 rh_G_precuneus_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 105 rh_G_rectus_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 106 rh_G_subcallosal_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 107 rh_G_temp_sup-G_T_transv_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 108 rh_G_temp_sup-Lateral_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 109 rh_G_temp_sup-Plan_polar_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 110 rh_G_temp_sup-Plan_tempo_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 111 rh_G_temporal_inf_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 112 rh_G_temporal_middle_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 113 rh_Lat_Fis-ant-Horizont_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 114 rh_Lat_Fis-ant-Vertical_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 115 rh_Lat_Fis-post_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 116 rh_Pole_occipital_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 117 rh_Pole_temporal_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 118 rh_S_calcarine_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 119 rh_S_central_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 120 rh_S_cingul-Marginalis_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 121 rh_S_circular_insula_ant_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 122 rh_S_circular_insula_inf_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 123 rh_S_circular_insula_sup_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 124 rh_S_collat_transv_ant_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 125 rh_S_collat_transv_post_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 126 rh_S_front_inf_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 127 rh_S_front_middle_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 128 rh_S_front_sup_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 129 rh_S_interm_prim-Jensen_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 130 rh_S_intrapariet&P_trans_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 131 rh_S_oc_middle&Lunatus_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 132 rh_S_oc_sup&transversal_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 133 rh_S_occipital_ant_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 134 rh_S_oc-temp_lat_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 135 rh_S_oc-temp_med&Lingual_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 136 rh_S_orbital_lateral_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 137 rh_S_orbital_med-olfact_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 138 rh_S_orbital-H_Shaped_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 139 rh_S_parieto_occipital_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 140 rh_S_pericallosal_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 141 rh_S_postcentral_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 142 rh_S_precentral-inf-part_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 143 rh_S_precentral-sup-part_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 144 rh_S_suborbital_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 145 rh_S_subparietal_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 146 rh_S_temporal_inf_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 147 rh_S_temporal_sup_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 148 rh_S_temporal_transverse_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 149 rh_MeanThickness_thickness :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 150 Left-Lateral-Ventricle :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 151 Left-Inf-Lat-Vent :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 152 Left-Cerebellum-White-Matter :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 153 Left-Cerebellum-Cortex :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 154 Left-Thalamus-Proper :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 155 Left-Caudate :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 156 Left-Putamen :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 157 Left-Pallidum :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 158 3rd-Ventricle :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 159 4th-Ventricle :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 160 Brain-Stem :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 161 Left-Hippocampus :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 162 Left-Amygdala :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 163 CSF :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 164 Left-Accumbens-area :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 165 Left-VentralDC :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 166 Left-vessel :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 167 Left-choroid-plexus :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 168 Right-Lateral-Ventricle :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 169 Right-Inf-Lat-Vent :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 170 Right-Cerebellum-White-Matter :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 171 Right-Cerebellum-Cortex :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 172 Right-Thalamus-Proper :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 173 Right-Caudate :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 174 Right-Putamen :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 175 Right-Pallidum :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 176 Right-Hippocampus :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 177 Right-Amygdala :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 178 Right-Accumbens-area :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 179 Right-VentralDC :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 180 Right-vessel :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 181 Right-choroid-plexus :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 182 SubCortGrayVol :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 183 TotalGrayVol :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 184 SupraTentorialVol :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 185 SupraTentorialVolNotVent :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n",
            "Running IDP 186 EstimatedTotalIntraCranialVol :\n",
            "Some sites missing from the training data. Adapting model\n",
            "Loading data ...\n",
            "Prediction by model  1 of 1\n",
            "Evaluating the model ...\n",
            "Evaluations Writing outputs ...\n",
            "Writing outputs ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75210821-ccb8-4bd2-82f3-641708811b21"
      },
      "source": [
        "### Preparing dummy data for plotting\n",
        "\n",
        "Now, we plot the centiles of variation estimated by the normative model. \n",
        "\n",
        "We do this by making use of a set of dummy covariates that span the whole range of the input space (for age) for a fixed value of the other covariates (e.g. sex) so that we can make predictions for these dummy data points, then plot them. We configure these dummy predictions using the same procedure as we used for the real data. We can use the same dummy data for all the IDPs we wish to plot"
      ],
      "id": "75210821-ccb8-4bd2-82f3-641708811b21"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d0743d8-28ca-4a14-8ef0-99bf40434b5b",
        "outputId": "7d4c8f2e-ca79-46e1-e5a8-0733503fde94"
      },
      "source": [
        "# which sex do we want to plot? \n",
        "sex = 1 # 1 = male 0 = female\n",
        "if sex == 1: \n",
        "    clr = 'blue';\n",
        "else:\n",
        "    clr = 'red'\n",
        "\n",
        "# create dummy data for visualisation\n",
        "print('configuring dummy data ...')\n",
        "xx = np.arange(xmin, xmax, 0.5)\n",
        "X0_dummy = np.zeros((len(xx), 2))\n",
        "X0_dummy[:,0] = xx\n",
        "X0_dummy[:,1] = sex\n",
        "\n",
        "# create the design matrix\n",
        "X_dummy = create_design_matrix(X0_dummy, xmin=xmin, xmax=xmax, site_ids=None, all_sites=site_ids_tr)\n",
        "\n",
        "# save the dummy covariates\n",
        "cov_file_dummy = os.path.join(out_dir,'cov_bspline_dummy_mean.txt')\n",
        "np.savetxt(cov_file_dummy, X_dummy)"
      ],
      "id": "2d0743d8-28ca-4a14-8ef0-99bf40434b5b",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "configuring dummy data ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "126323a3-2270-4796-97c4-94629730ddf7"
      },
      "source": [
        "### Plotting the normative models\n",
        "\n",
        "Now we loop through the IDPs, plotting each one separately. The outputs of this step are a set of quantitative regression metrics for each IDP and a set of centile curves which we plot the test data against. \n",
        "\n",
        "This part of the code is relatively complex because we need to keep track of many quantities for the plotting. We also need to remember whether the data need to be warped or not. By default in PCNtoolkit, predictions in the form of `yhat, s2` are always in the warped (Gaussian) space. If we want predictions in the input (non-Gaussian) space, then we need to warp them with the inverse of the estimated warping function. This can be done using the function `nm.blr.warp.warp_predictions()`. \n",
        "\n",
        "**Note:** it is necessary to update the intercept for each of the sites. For purposes of visualisation, here we do this by adjusting the median of the data to match the dummy predictions, but note that all the quantitative metrics are estimated using the predictions that are adjusted properly using a learned offset (or adjusted using a hold-out adaptation set, as above). Note also that for the calibration data we require at least two data points of the same sex in each site to be able to estimate the variance. Of course, in a real example, you would want many more than just two since we need to get a reliable estimate of the variance for each site. "
      ],
      "id": "126323a3-2270-4796-97c4-94629730ddf7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdd68cc6-212b-4149-b86a-24e842078e1a"
      },
      "source": [
        "sns.set(style='whitegrid')\n",
        "\n",
        "for idp_num, idp in enumerate(idp_ids): \n",
        "    print('Running IDP', idp_num, idp, ':')\n",
        "    idp_dir = os.path.join(out_dir, idp)\n",
        "    os.chdir(idp_dir)\n",
        "    \n",
        "    # load the true data points\n",
        "    yhat_te = load_2d(os.path.join(idp_dir, 'yhat_predict.txt'))\n",
        "    s2_te = load_2d(os.path.join(idp_dir, 'ys2_predict.txt'))\n",
        "    y_te = load_2d(os.path.join(idp_dir, 'resp_te.txt'))\n",
        "            \n",
        "    # set up the covariates for the dummy data\n",
        "    print('Making predictions with dummy covariates (for visualisation)')\n",
        "    yhat, s2 = predict(cov_file_dummy, \n",
        "                       alg = 'blr', \n",
        "                       respfile = None, \n",
        "                       model_path = os.path.join(idp_dir,'Models'), \n",
        "                       outputsuffix = '_dummy')\n",
        "    \n",
        "    # load the normative model\n",
        "    with open(os.path.join(idp_dir,'Models', 'NM_0_0_estimate.pkl'), 'rb') as handle:\n",
        "        nm = pickle.load(handle) \n",
        "    \n",
        "    # get the warp and warp parameters\n",
        "    W = nm.blr.warp\n",
        "    warp_param = nm.blr.hyp[1:nm.blr.warp.get_n_params()+1] \n",
        "        \n",
        "    # first, we warp predictions for the true data and compute evaluation metrics\n",
        "    med_te = W.warp_predictions(np.squeeze(yhat_te), np.squeeze(s2_te), warp_param)[0]\n",
        "    med_te = med_te[:, np.newaxis]\n",
        "    print('metrics:', evaluate(y_te, med_te))\n",
        "    \n",
        "    # then, we warp dummy predictions to create the plots\n",
        "    med, pr_int = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2), warp_param)\n",
        "    \n",
        "    # extract the different variance components to visualise\n",
        "    beta, junk1, junk2 = nm.blr._parse_hyps(nm.blr.hyp, X_dummy)\n",
        "    s2n = 1/beta # variation (aleatoric uncertainty)\n",
        "    s2s = s2-s2n # modelling uncertainty (epistemic uncertainty)\n",
        "    \n",
        "    # plot the data points\n",
        "    y_te_rescaled_all = np.zeros_like(y_te)\n",
        "    for sid, site in enumerate(site_ids_te):\n",
        "        # plot the true test data points \n",
        "        if all(elem in site_ids_tr for elem in site_ids_te):\n",
        "            # all data in the test set are present in the training set\n",
        "            \n",
        "            # first, we select the data points belonging to this particular site\n",
        "            idx = np.where(np.bitwise_and(X_te[:,2] == sex, X_te[:,sid+len(cols_cov)+1] !=0))[0]\n",
        "            if len(idx) == 0:\n",
        "                print('No data for site', sid, site, 'skipping...')\n",
        "                continue\n",
        "            \n",
        "            # then directly adjust the data\n",
        "            idx_dummy = np.bitwise_and(X_dummy[:,1] > X_te[idx,1].min(), X_dummy[:,1] < X_te[idx,1].max())\n",
        "            y_te_rescaled = y_te[idx] - np.median(y_te[idx]) + np.median(med[idx_dummy])\n",
        "        else:\n",
        "            # we need to adjust the data based on the adaptation dataset \n",
        "            \n",
        "            # first, select the data point belonging to this particular site\n",
        "            idx = np.where(np.bitwise_and(X_te[:,2] == sex, (df_te['site'] == site).to_numpy()))[0]\n",
        "            \n",
        "            # load the adaptation data\n",
        "            y_ad = load_2d(os.path.join(idp_dir, 'resp_ad.txt'))\n",
        "            X_ad = load_2d(os.path.join(idp_dir, 'cov_bspline_ad.txt'))\n",
        "            idx_a = np.where(np.bitwise_and(X_ad[:,2] == sex, (df_ad['site'] == site).to_numpy()))[0]\n",
        "            if len(idx) < 2 or len(idx_a) < 2:\n",
        "                print('Insufficent data for site', sid, site, 'skipping...')\n",
        "                continue\n",
        "            \n",
        "            # adjust and rescale the data\n",
        "            y_te_rescaled, s2_rescaled = nm.blr.predict_and_adjust(nm.blr.hyp, \n",
        "                                                                   X_ad[idx_a,:], \n",
        "                                                                   np.squeeze(y_ad[idx_a]), \n",
        "                                                                   Xs=None, \n",
        "                                                                   ys=np.squeeze(y_te[idx]))\n",
        "        # plot the (adjusted) data points\n",
        "        plt.scatter(X_te[idx,1], y_te_rescaled, s=4, color=clr, alpha = 0.1)\n",
        "       \n",
        "    # plot the median of the dummy data\n",
        "    plt.plot(xx, med, clr)\n",
        "    \n",
        "    # fill the gaps in between the centiles\n",
        "    junk, pr_int25 = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2), warp_param, percentiles=[0.25,0.75])\n",
        "    junk, pr_int95 = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2), warp_param, percentiles=[0.05,0.95])\n",
        "    junk, pr_int99 = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2), warp_param, percentiles=[0.01,0.99])\n",
        "    plt.fill_between(xx, pr_int25[:,0], pr_int25[:,1], alpha = 0.1,color=clr)\n",
        "    plt.fill_between(xx, pr_int95[:,0], pr_int95[:,1], alpha = 0.1,color=clr)\n",
        "    plt.fill_between(xx, pr_int99[:,0], pr_int99[:,1], alpha = 0.1,color=clr)\n",
        "            \n",
        "    # make the width of each centile proportional to the epistemic uncertainty\n",
        "    junk, pr_int25l = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2-0.5*s2s), warp_param, percentiles=[0.25,0.75])\n",
        "    junk, pr_int95l = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2-0.5*s2s), warp_param, percentiles=[0.05,0.95])\n",
        "    junk, pr_int99l = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2-0.5*s2s), warp_param, percentiles=[0.01,0.99])\n",
        "    junk, pr_int25u = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2+0.5*s2s), warp_param, percentiles=[0.25,0.75])\n",
        "    junk, pr_int95u = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2+0.5*s2s), warp_param, percentiles=[0.05,0.95])\n",
        "    junk, pr_int99u = W.warp_predictions(np.squeeze(yhat), np.squeeze(s2+0.5*s2s), warp_param, percentiles=[0.01,0.99])    \n",
        "    plt.fill_between(xx, pr_int25l[:,0], pr_int25u[:,0], alpha = 0.3,color=clr)\n",
        "    plt.fill_between(xx, pr_int95l[:,0], pr_int95u[:,0], alpha = 0.3,color=clr)\n",
        "    plt.fill_between(xx, pr_int99l[:,0], pr_int99u[:,0], alpha = 0.3,color=clr)\n",
        "    plt.fill_between(xx, pr_int25l[:,1], pr_int25u[:,1], alpha = 0.3,color=clr)\n",
        "    plt.fill_between(xx, pr_int95l[:,1], pr_int95u[:,1], alpha = 0.3,color=clr)\n",
        "    plt.fill_between(xx, pr_int99l[:,1], pr_int99u[:,1], alpha = 0.3,color=clr)\n",
        "\n",
        "    # plot actual centile lines\n",
        "    plt.plot(xx, pr_int25[:,0],color=clr, linewidth=0.5)\n",
        "    plt.plot(xx, pr_int25[:,1],color=clr, linewidth=0.5)\n",
        "    plt.plot(xx, pr_int95[:,0],color=clr, linewidth=0.5)\n",
        "    plt.plot(xx, pr_int95[:,1],color=clr, linewidth=0.5)\n",
        "    plt.plot(xx, pr_int99[:,0],color=clr, linewidth=0.5)\n",
        "    plt.plot(xx, pr_int99[:,1],color=clr, linewidth=0.5)\n",
        "    \n",
        "    plt.xlabel('Age')\n",
        "    plt.ylabel(idp) \n",
        "    plt.title(idp)\n",
        "    plt.xlim((0,90))\n",
        "    plt.savefig(os.path.join(idp_dir, 'centiles_' + str(sex)),  bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "os.chdir(out_dir)"
      ],
      "id": "cdd68cc6-212b-4149-b86a-24e842078e1a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMUyOWOLmU1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0111629-5919-40a4-cde7-dbf5eaf9f692"
      },
      "source": [
        "# explore an example output folder of a single model (one ROI)\n",
        "ls rh_MeanThickness_thickness/"
      ],
      "id": "OMUyOWOLmU1b",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "centiles_1.png      MSLL_predict.txt  RMSE_predict.txt  yhat_predict.txt\n",
            "cov_bspline_ad.txt  pRho_predict.txt  sitenum_ad.txt    ys2_dummy.pkl\n",
            "cov_bspline_te.txt  resp_ad.txt       sitenum_te.txt    ys2_predict.txt\n",
            "EXPV_predict.txt    resp_te.txt       SMSE_predict.txt  Z_predict.txt\n",
            "\u001b[0m\u001b[01;34mModels\u001b[0m/             Rho_predict.txt   yhat_dummy.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJIFVhQ5zKBw",
        "outputId": "d9f6e492-7ec1-40af-bcb4-1ea94eaed09e"
      },
      "source": [
        "# check that the number of deviation scores matches the number of subjects in the test set\n",
        "# there should be one deviation score per subject (one line per subject), so we can\n",
        "# verify by counting the line numbers in the Z_predict.txt file\n",
        "! cat rh_MeanThickness_thickness/Z_predict.txt | wc"
      ],
      "id": "TJIFVhQ5zKBw",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    631     631   16052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZEs7Ej4-qGi"
      },
      "source": [
        "The deviation scores are output as a text file in separate folders. We want to summarize the deviation scores across all models estimates so we can organize them into a single file, and merge the deviation scores into the original data file. "
      ],
      "id": "hZEs7Ej4-qGi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-OauNfc5Jrx"
      },
      "source": [
        "! mkdir deviation_scores"
      ],
      "id": "L-OauNfc5Jrx",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEgnixDd5KgK"
      },
      "source": [
        "! for i in *; do if [[ -e ${i}/Z_predict.txt ]]; then cp ${i}/Z_predict.txt deviation_scores/${i}_Z_predict.txt; fi; done"
      ],
      "id": "ZEgnixDd5KgK",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10gP5z-t7-ZC"
      },
      "source": [
        "z_dir = '/content/braincharts/models/lifespan_57K_82sites/deviation_scores/'\n",
        "filelist = [name for name in os.listdir(z_dir)]"
      ],
      "id": "10gP5z-t7-ZC",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2GAFv5F8TFa"
      },
      "source": [
        "os.chdir(z_dir)\n",
        "Z_df = pd.concat([pd.read_csv(item, names=[item[:-4]]) for item in filelist], axis=1)"
      ],
      "id": "Q2GAFv5F8TFa",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHcx4vsj8eMf"
      },
      "source": [
        "df_te.reset_index(inplace=True)"
      ],
      "id": "FHcx4vsj8eMf",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9werTREu8c0P"
      },
      "source": [
        "Z_df['sub_id'] = df_te['sub_id']"
      ],
      "id": "9werTREu8c0P",
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgAJ86wy9U5A"
      },
      "source": [
        "df_te_Z = pd.merge(df_te, Z_df, on='sub_id', how='inner')"
      ],
      "id": "WgAJ86wy9U5A",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn6I12zh9t1g"
      },
      "source": [
        "df_te_Z.to_csv('fcon1000_te_Z.csv', index=False)"
      ],
      "id": "bn6I12zh9t1g",
      "execution_count": 65,
      "outputs": []
    }
  ]
}